name: DevSecOps Pipeline
on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]

jobs:
  build-deploy-scan:
    runs-on: self-hosted
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Cache Python dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles("requirements.txt") }}

      - name: Set up Python virtual environment
        run: |
          python3 -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          rm -rf ~/.cache/pip/*
          pip install -r requirements.txt
          pip install httpx==0.27.2
          pip install openai==1.51.0
          checkov --version || pip install checkov==3.2.258
          deactivate

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Bootstrap builder
        run: docker buildx inspect --bootstrap

      - name: Login to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ secrets.GHCR_USERNAME }}
          password: ${{ secrets.GHCR_TOKEN }}

      - name: Build & push Docker image
        id: meta
        run: |
          GIT_SHA=$(git rev-parse --short HEAD )
          echo "image_tag=$GIT_SHA" >> $GITHUB_OUTPUT
          docker build -t ${{ secrets.REGISTRY }}/app:$GIT_SHA -t ${{ secrets.REGISTRY }}/app:latest app
          docker push ${{ secrets.REGISTRY }}/app:$GIT_SHA
          docker push ${{ secrets.REGISTRY }}/app:latest
          sed -i "s#\${IMAGE_TAG}#$GIT_SHA#g" app/k8s/deployment.yaml

      - name: Deploy app to k3s
        run: |
          echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > kubeconfig
          chmod 600 kubeconfig
          export KUBECONFIG=kubeconfig
          kubectl apply -f app/k8s/namespace.yaml
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=ghcr.io \
            --docker-username=${{ secrets.GHCR_USERNAME }} \
            --docker-password=${{ secrets.GHCR_TOKEN }} \
            --namespace=app-test \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl apply -f app/k8s/deployment.yaml
          kubectl apply -f app/k8s/service.yaml
          kubectl apply -f app/k8s/ingress.yaml
          kubectl apply -f app/k8s/networkpolicy.yaml # <--- ADDED THIS LINE
          kubectl apply -f policies/gatekeeper-templates/deny-high-risk.yaml
          kubectl apply -f policies/gatekeeper-constraints/deny-high-risk.yaml
          sleep 10
          kubectl rollout status -n app-test deploy/demo-app --timeout=600s

      - name: Clear runner cache
        run: |
          rm -rf $HOME/.local/bin/*
          rm -rf $HOME/.cache/*

      - name: Install local scanners
        continue-on-error: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          mkdir -p $HOME/.local/bin
          export PATH=$HOME/.local/bin:$PATH
          ZAP_VERSION="2.15.0"
          wget https://github.com/zaproxy/zaproxy/releases/download/v$ZAP_VERSION/ZAP_${ZAP_VERSION}_Linux.tar.gz -O zap.tar.gz
          tar -xzf zap.tar.gz -C $HOME/.local/bin
          mv $HOME/.local/bin/ZAP_${ZAP_VERSION} $HOME/.local/bin/zap
          rm -f zap.tar.gz
          TRIVY_VERSION="0.66.0"
          curl -L https://github.com/aquasecurity/trivy/releases/download/v$TRIVY_VERSION/trivy_${TRIVY_VERSION}_Linux-64bit.tar.gz -o trivy.tar.gz
          tar -xzf trivy.tar.gz -C $HOME/.local/bin
          chmod +x $HOME/.local/bin/trivy
          trivy --version && echo "Trivy binary is functional" || echo "trivy_docker_fallback=true" >> $GITHUB_ENV
          rm -f trivy.tar.gz
          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b $HOME/.local/bin
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Verify scanner installation
        continue-on-error: true
        run: |
          export PATH=$HOME/.local/bin:$PATH
          source venv/bin/activate
          semgrep --version || { echo "Semgrep installation failed"; exit 1; }
          checkov --version || { echo "Checkov installation failed"; exit 1; }
          $HOME/.local/bin/zap/zap.sh -version || { echo "OWASP ZAP installation failed"; exit 1; }
          deactivate
          if [ "${{ env.trivy_docker_fallback }}" != "true" ]; then
            trivy --version || echo "trivy_docker_fallback=true" >> $GITHUB_ENV
          fi
          syft --version || { echo "Syft installation failed"; exit 1; }
          conftest --version || true
          gitleaks version || { echo "Gitleaks installation failed"; exit 1; }

      - name: Run SAST scan
        continue-on-error: true
        run: |
          export PATH=$HOME/.local/bin:$PATH
          source venv/bin/activate
          semgrep --config scanners/semgrep_rules.yml --json > sast.json || echo "[]" > sast.json
          deactivate

      - name: Run Secrets scan
        continue-on-error: true
        run: |
          export PATH=$HOME/.local/bin:$PATH
          gitleaks detect --source . --report-path secrets.json || echo "[]" > secrets.json

      - name: Run IaC scan
        continue-on-error: true
        run: |
          export PATH=$HOME/.local/bin:$PATH
          source venv/bin/activate
          mkdir -p scanners
          if [ ! -f scanners/checkov.yml ]; then
            printf \"{\"results\": {\"failed_checks\": [], \"passed_checks\": [], \"skipped_checks\": []}}\" > scanners/results.json
            deactivate
            exit 0
          fi
          checkov -d app/k8s --config-file scanners/checkov.yml --output json --output-file-path scanners/results.json 2> checkov_error.log || \
            printf \"{\"results\": {\"failed_checks\": [], \"passed_checks\": [], \"skipped_checks\": []}}\" > scanners/results.json
          deactivate

      - name: Run SCA scan
        continue-on-error: true
        run: |
          export PATH=$HOME/.local/bin:$PATH
          if [ \"${{ env.trivy_docker_fallback }}\" != \"true\" ]; then
            trivy fs --format json --output sca.json . || echo \"[]\" > sca.json
          else
            docker run --rm -v $(pwd ):/workspace -w /workspace aquasec/trivy:0.66.0 fs --format json --output sca.json . || echo \"[]\" > sca.json
          fi

      - name: Generate SBOM
        continue-on-error: true
        run: |
          export PATH=$HOME/.local/bin:$PATH
          syft dir:app -o json > sbom.json || echo \"[]\" > sbom.json

      - name: Wait for service to be ready
        continue-on-error: true
        run: |
          export KUBECONFIG=kubeconfig
          kubectl wait --for=condition=ready pod -n app-test -l app=demo-app --timeout=300s
          kubectl get service demo-app-svc -n app-test
          kubectl get ingress -n app-test
          kubectl describe ingress demo-app-ing -n app-test
          
          # Add a robust wait for the application to be truly accessible
          echo "Waiting for application to be accessible via service..."
          SERVICE_IP=$(kubectl get service demo-app-svc -n app-test -o jsonpath=".spec.clusterIP")
          SERVICE_DNS="demo-app-svc.app-test.svc.cluster.local"
          
          for i in $(seq 1 30); do
            if curl -s -o /dev/null -w "%{http_code}" "http://$SERVICE_IP:80" | grep "200"; then
              echo "Application accessible via service IP: $SERVICE_IP:80"
              break
            elif curl -s -o /dev/null -w "%{http_code}" "http://$SERVICE_DNS:80" | grep "200"; then
              echo "Application accessible via service DNS: $SERVICE_DNS:80"
              break
            else
              echo "Attempt $i: Application not yet accessible. Retrying in 5 seconds..."
              sleep 5
            fi
            if [ $i -eq 30 ]; then
              echo "Application did not become accessible within timeout."
              exit 1
            fi
          done

      - name: Test internal connectivity before ZAP
        run: |
          export KUBECONFIG=kubeconfig
          
          # Get the service IP
          SERVICE_IP=$(kubectl get service demo-app-svc -n app-test -o jsonpath=".spec.clusterIP" )
          echo "Service IP: $SERVICE_IP"
          
          # Check if pods are running
          echo "=== Pod Status ==="
          kubectl get pods -n app-test -l app=demo-app -o wide
          
          # Check service details
          echo "=== Service Details ==="
          kubectl describe service demo-app-svc -n app-test
          
          # Check endpoints
          echo "=== Service Endpoints ==="
          kubectl get endpoints demo-app-svc -n app-test -o wide
          
          # Test connectivity from within cluster
          echo "=== Testing Internal Connectivity ==="
          kubectl run test-connectivity --image=curlimages/curl:latest --rm -i --restart=Never -n app-test -- sh -c "\
            echo \"Testing service IP: $SERVICE_IP:80\"; \
            curl -v --fail --connect-timeout 10 --max-time 30 \"http://$SERVICE_IP:80\" || echo \"Service IP failed\"; \
            echo \"Testing service DNS:\"; \
            curl -v --fail --connect-timeout 10 --max-time 30 \"http://demo-app-svc.app-test.svc.cluster.local:80\" || echo \"Service DNS failed\"; \
          " || echo "Connectivity test failed"

      - name: Create ZAP ConfigMap and Job YAML
        run: |
          export KUBECONFIG=kubeconfig
          mkdir -p scanners
          
          # Get the actual service IP dynamically
          SERVICE_IP=$(kubectl get service demo-app-svc -n app-test -o jsonpath=".spec.clusterIP" 2>/dev/null )
          if [ -z "$SERVICE_IP" ]; then
            echo "Could not get service IP, using DNS name"
            TARGET_URL="http://demo-app-svc.app-test.svc.cluster.local:80"
          else
            echo "Using service IP: $SERVICE_IP"
            TARGET_URL="http://$SERVICE_IP:80"
          fi
          
          echo "$TARGET_URL" > scanners/target-url
          echo "-c /zap/wrk/config/zap_config.yml" > scanners/zap-options
          kubectl create configmap zap-config \
            --from-file=target-url=scanners/target-url \
            --from-file=zap-options=scanners/zap-options \
            -n app-test --dry-run=client -o yaml | kubectl apply -f -
          
          # Update the zap-job.yaml with the dynamic target
          # The zap-job.yaml is now configured to use the TARGET_URL directly
          # No sed replacement needed here if the zap-job.yaml is properly templated
          # For this example, we assume zap-job.yaml will be updated to directly use the TARGET_URL from the configmap or environment variable.
          
          kubectl delete job zap-scan -n app-test --ignore-not-found=true
          sleep 5
          kubectl apply -f scanners/zap-job.yaml
          
          echo "Waiting for ZAP job to complete, target: $TARGET_URL"
          kubectl wait --for=condition=complete job/zap-scan -n app-test --timeout=1800s || {
            echo "ZAP scan timed out or failed - debugging:"
            kubectl describe job zap-scan -n app-test
            kubectl get pods -n app-test -l job-name=zap-scan
            POD_NAME=$(kubectl get pod -n app-test -l job-name=zap-scan -o jsonpath=".items[0].metadata.name" 2>/dev/null )
            if [ ! -z "$POD_NAME" ]; then
              echo "ZAP pod logs:"
              kubectl logs -n app-test $POD_NAME --tail=100
            fi
          }
          
          POD_NAME=$(kubectl get pod -n app-test -l job-name=zap-scan -o jsonpath=".items[0].metadata.name" 2>/dev/null)
          if [ ! -z "$POD_NAME" ]; then
            kubectl cp app-test/$POD_NAME:/zap/wrk/zap-report.json zap-report.json || echo \"{\"site\": []}\" > zap-report.json
            kubectl cp app-test/$POD_NAME:/zap/wrk/zap-report.html zap-report.html || echo "<html><body>No ZAP report generated</body></html>" > zap-report.html
          else
            echo \"{\"site\": []}\" > zap-report.json
            echo "<html><body>No ZAP report generated</body></html>" > zap-report.html
          fi

      - name: Merge findings
        run: |
          source venv/bin/activate
          python3 risk_model/inference/merge_findings.py \
            --sast sast.json \
            --secrets secrets.json \
            --iac scanners/results.json \
            --sca sca.json \
            --dast zap-report.json \
            --out findings.jsonl
          deactivate

      - name: AI inference + HTML report
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          source venv/bin/activate
          python3 risk_model/inference/infer.py \
            --artifacts-dir artifacts \
            --in findings.jsonl \
            --out enriched.jsonl \
            --report report.html
          deactivate

      - name: Evaluate adaptive gate
        id: gate
        run: |
          source venv/bin/activate
          python3 adaptive/evaluate_gate.py > gate.json
          THRESHOLD=$(jq -r .adaptive_threshold gate.json)
          echo "threshold=$THRESHOLD" >> $GITHUB_OUTPUT
          deactivate

      - name: Check gate
        env:
          THRESH: ${{ steps.gate.outputs.threshold }}
        run: |
          source venv/bin/activate
          python3 adaptive/check_gate.py
          deactivate

      - name: Run Red Team tests
        continue-on-error: true
        run: |
          export KUBECONFIG=kubeconfig
          source venv/bin/activate
          TARGET_HOST=$(kubectl get ingress -n app-test -o jsonpath=".items[0].status.loadBalancer.ingress[0].hostname" 2>/dev/null || echo "staging.hamza-builds.info")
          python3 redteam/run_attacks.py --target "http://$TARGET_HOST:80" --out redteam_results.json || echo "[]" > redteam_results.json
          deactivate

      - name: Debug app connectivity
        if: always( )
        run: |
          export KUBECONFIG=kubeconfig
          echo "=== Debugging Application Pod ==="
          POD_NAME=$(kubectl get pods -n app-test -l app=demo-app -o jsonpath=".items[0].metadata.name")
          if [ ! -z "$POD_NAME" ]; then
            echo "Application Pod Logs:"
            kubectl logs -n app-test $POD_NAME --tail=200
            echo "Application Pod Describe:"
            kubectl describe pod -n app-test $POD_NAME
            
            echo "=== In-Pod Connectivity and Listening Ports ==="
            # Install curl and net-tools inside the pod for debugging
            # Use 'apk add' for Alpine-based images (common in BusyBox, Python-slim) or 'apt-get' for Debian-based
            kubectl exec -n app-test $POD_NAME -- sh -c "if command -v apk &> /dev/null; then apk add --no-cache curl net-tools; elif command -v apt-get &> /dev/null; then apt-get update && apt-get install -y curl net-tools; else echo \"Neither apk nor apt-get found. Cannot install debugging tools.\"; fi"
            
            echo "Listening ports inside pod:"
            kubectl exec -n app-test $POD_NAME -- netstat -tulnp || echo "netstat not found or failed"
            
            echo "Curl 127.0.0.1:8080 from inside pod:"
            kubectl exec -n app-test $POD_NAME -- curl -v --fail --connect-timeout 5 --max-time 10 http://127.0.0.1:8080 || echo "Curl to localhost:8080 failed"
            
            echo "Curl service IP from inside pod:"
            SERVICE_IP_INTERNAL=$(kubectl get service demo-app-svc -n app-test -o jsonpath=".spec.clusterIP" )
            kubectl exec -n app-test $POD_NAME -- curl -v --fail --connect-timeout 5 --max-time 10 http://$SERVICE_IP_INTERNAL:80 || echo "Curl to service IP from inside pod failed"

          else
            echo "No application pod found."
          fi
          echo "Application Deployment Describe:"
          kubectl describe deployment -n app-test demo-app

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        if: always( )
        with:
          name: security-report-${{ github.run_number }}
          path: |
            report.html
            findings.jsonl
            enriched.jsonl
            redteam_results.json
            zap-report.json
            zap-report.html
            scanners/results.json
            sast.json
            secrets.json
            sca.json
            sbom.json
            gate.json

      - name: Clean up kubeconfig
        if: always()
        run: rm -f kubeconfig
