name: DevSecOps Pipeline
on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]

jobs:
  build-deploy-scan:
    runs-on: self-hosted
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Cache Python dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}

      - name: Set up Python virtual environment
        run: |
          python3 -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          echo "=== Clearing pip cache to ensure fresh dependency installation ==="
          rm -rf ~/.cache/pip/*
          pip install -r requirements.txt
          echo "=== Ensuring httpx==0.27.2 for openai compatibility ==="
          pip install httpx==0.27.2
          echo "=== Installing openai==1.51.0 ==="
          pip install openai==1.51.0
          echo "=== Verifying installed packages ==="
          pip list
          echo "=== Checking openai version ==="
          python3 -c "import openai; print(openai.__version__)" || {
            echo "Failed to import openai. Ensuring httpx and reinstalling openai==1.51.0."
            pip install httpx==0.27.2
            pip install openai==1.51.0
            python3 -c "import openai; print(openai.__version__)"
          }
          echo "=== Checking httpx version ==="
          python3 -c "import httpx; print(httpx.__version__)" || {
            echo "httpx not found, installing httpx==0.27.2."
            pip install httpx==0.27.2
            python3 -c "import httpx; print(httpx.__version__)"
          }
          echo "=== Checking checkov version ==="
          checkov --version || {
            echo "Failed to verify checkov version. Reinstalling checkov==3.2.258."
            pip install checkov==3.2.258
            checkov --version
          }
          echo "=== Checking OpenAI class import ==="
          python3 -c "from openai import OpenAI; print('OpenAI import successful')" || {
            echo "OpenAI class import failed."
            exit 1
          }
          echo "=== Checking for dependency conflicts ==="
          pip check || {
            echo "Dependency conflicts detected, continuing with warning."
            echo "Please review pip check output for potential issues."
          }
          deactivate

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Bootstrap builder
        run: docker buildx inspect --bootstrap

      - name: Login to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ secrets.GHCR_USERNAME }}
          password: ${{ secrets.GHCR_TOKEN }}

      - name: Build & push Docker image
        id: meta
        run: |
          GIT_SHA=$(git rev-parse --short HEAD)
          echo "image_tag=$GIT_SHA" >> $GITHUB_OUTPUT
          echo "Building image: ${{ secrets.REGISTRY }}/app:$GIT_SHA"
          docker build -t ${{ secrets.REGISTRY }}/app:$GIT_SHA -t ${{ secrets.REGISTRY }}/app:latest app
          if [ $? -ne 0 ]; then
            echo "Docker build failed"
            exit 1
          fi
          echo "Pushing image: ${{ secrets.REGISTRY }}/app:$GIT_SHA"
          docker push ${{ secrets.REGISTRY }}/app:$GIT_SHA
          if [ $? -ne 0 ]; then
            echo "Docker push failed for tag $GIT_SHA"
            exit 1
          fi
          echo "Pushing image: ${{ secrets.REGISTRY }}/app:latest"
          docker push ${{ secrets.REGISTRY }}/app:latest
          if [ $? -ne 0 ]; then
            echo "Docker push failed for tag latest"
            exit 1
          fi
          sed -i "s|\${IMAGE_TAG}|$GIT_SHA|g" app/k8s/deployment.yaml
          echo "=== Updated deployment.yaml ==="
          cat app/k8s/deployment.yaml

      - name: Deploy app to k3s
        run: |
          echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > kubeconfig
          chmod 600 kubeconfig
          export KUBECONFIG=kubeconfig
          
          # Apply namespace first
          kubectl apply -f app/k8s/namespace.yaml
          
          # Create image pull secret for GHCR
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=ghcr.io \
            --docker-username=${{ secrets.GHCR_USERNAME }} \
            --docker-password=${{ secrets.GHCR_TOKEN }} \
            --namespace=app-test \
            --dry-run=client -o yaml | kubectl apply -f -
          
          # Apply other manifests
          kubectl apply -f app/k8s/deployment.yaml
          echo "=== Deployment YAML ==="
          cat app/k8s/deployment.yaml
          kubectl apply -f app/k8s/service.yaml
          kubectl apply -f app/k8s/ingress.yaml
          kubectl apply -f policies/gatekeeper-templates/deny-high-risk.yaml
          kubectl apply -f policies/gatekeeper-constraints/deny-high-risk.yaml
          
          # Wait a moment for pods to start
          sleep 10
          
          # Debug information
          echo "=== Deployment Status ==="
          kubectl get deployment demo-app -n app-test -o wide
          echo "=== Pod Status ==="
          kubectl get pods -n app-test -o wide
          echo "=== Pod Descriptions ==="
          kubectl describe pods -n app-test -l app=demo-app
          echo "=== Recent Events ==="
          kubectl get events -n app-test --sort-by='.lastTimestamp' | tail -10
          echo "=== Ingress Debug ==="
          kubectl describe ingress demo-app-ing -n app-test
          
          # Wait for rollout with extended timeout and better error handling
          if ! kubectl rollout status -n app-test deploy/demo-app --timeout=600s; then
            echo "=== DEPLOYMENT FAILED - DETAILED DEBUG INFO ==="
            echo "=== Deployment Description ==="
            kubectl describe deployment demo-app -n app-test
            echo "=== Pod Descriptions ==="
            kubectl describe pods -n app-test -l app=demo-app
            echo "=== Pod Logs ==="
            kubectl logs -n app-test -l app=demo-app --tail=50 --all-containers=true --previous=false || true
            echo "=== Previous Pod Logs (if any) ==="
            kubectl logs -n app-test -l app=demo-app --tail=20 --all-containers=true --previous=true || true
            echo "=== All Events ==="
            kubectl get events -n app-test --sort-by='.lastTimestamp'
            exit 1
          fi
          
          echo "=== Deployment Successful ==="
          kubectl get pods -n app-test -l app=demo-app

      - name: Clear runner cache
        run: |
          rm -rf $HOME/.local/bin/*
          rm -rf $HOME/.cache/*

      - name: Install local scanners
        continue-on-error: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          mkdir -p $HOME/.local/bin
          export PATH=$HOME/.local/bin:$PATH
          ZAP_VERSION="2.15.0"
          wget https://github.com/zaproxy/zaproxy/releases/download/v$ZAP_VERSION/ZAP_${ZAP_VERSION}_Linux.tar.gz -O zap.tar.gz
          tar -xzf zap.tar.gz -C $HOME/.local/bin
          mv $HOME/.local/bin/ZAP_${ZAP_VERSION} $HOME/.local/bin/zap
          rm -f zap.tar.gz
          TRIVY_VERSION="0.66.0"
          curl -L https://github.com/aquasecurity/trivy/releases/download/v$TRIVY_VERSION/trivy_${TRIVY_VERSION}_Linux-64bit.tar.gz -o trivy.tar.gz
          tar -xzf trivy.tar.gz -C $HOME/.local/bin
          chmod +x $HOME/.local/bin/trivy
          trivy --version && echo "Trivy binary is functional" || echo "trivy_docker_fallback=true" >> $GITHUB_ENV
          rm -f trivy.tar.gz
          curl -sSL https://github.com/gitleaks/gitleaks/releases/latest/download/gitleaks_linux_amd64.tar.gz -o gitleaks.tar.gz
          tar -xzf gitleaks.tar.gz -C $HOME/.local/bin
          rm -f gitleaks.tar.gz
          curl -L https://github.com/open-policy-agent/conftest/releases/latest/download/conftest_0.60.0_Linux_x86_64.tar.gz -o conftest.tar.gz
          tar -xzf conftest.tar.gz
          mv conftest $HOME/.local/bin/conftest
          rm -f conftest.tar.gz
          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b $HOME/.local/bin
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Verify scanner installation
        continue-on-error: true
        run: |
          export PATH=$HOME/.local/bin:$PATH
          source venv/bin/activate
          semgrep --version || { echo "Semgrep installation failed"; exit 1; }
          checkov --version || { echo "Checkov installation failed"; exit 1; }
          $HOME/.local/bin/zap/zap.sh -version || { echo "OWASP ZAP installation failed"; exit 1; }
          deactivate
          if [ "${{ env.trivy_docker_fallback }}" != "true" ]; then
            trivy --version || echo "trivy_docker_fallback=true" >> $GITHUB_ENV
          fi
          syft --version || { echo "Syft installation failed"; exit 1; }
          conftest --version || true
          gitleaks version || { echo "Gitleaks installation failed"; exit 1; }

      - name: Run SAST scan
        continue-on-error: true
        run: |
          export PATH=$HOME/.local/bin:$PATH
          source venv/bin/activate
          semgrep --config scanners/semgrep_rules.yml --json > sast.json || echo "[]" > sast.json
          deactivate

      - name: Run Secrets scan
        continue-on-error: true
        run: |
          export PATH=$HOME/.local/bin:$PATH
          gitleaks detect --source . --report-path secrets.json || echo "[]" > secrets.json

      - name: Run IaC scan
        continue-on-error: true
        run: |
          export PATH=$HOME/.local/bin:$PATH
          source venv/bin/activate
          echo "Checking app/k8s directory"
          ls -l app/k8s || echo "app/k8s directory not found or empty"
          echo "Checking scanners/checkov.yml"
          if [ ! -f scanners/checkov.yml ]; then
            echo "scanners/checkov.yml not found, creating empty results"
            echo '{"results": {"failed_checks": [], "passed_checks": [], "skipped_checks": []}}' > checkov-report.json
            deactivate
            exit 0
          fi
          cat scanners/checkov.yml
          checkov -d app/k8s --config-file scanners/checkov.yml --output json --output-file-path checkov-report.json 2> checkov_error.log
          if [ $? -ne 0 ]; then
            echo "Checkov scan failed, creating empty results"
            echo '{"results": {"failed_checks": [], "passed_checks": [], "skipped_checks": []}}' > checkov-report.json
            echo "Checkov Error Log:"
            cat checkov_error.log
          fi
          # Ensure the file exists before trying to cat it
          if [ -f checkov-report.json ]; then
            echo "Checkov scan output:"
            cat checkov-report.json
          else
            echo "Creating empty checkov report"
            echo '{"results": {"failed_checks": [], "passed_checks": [], "skipped_checks": []}}' > checkov-report.json
          fi
          deactivate

      - name: Run SCA scan
        continue-on-error: true
        run: |
          export PATH=$HOME/.local/bin:$PATH
          if [ "${{ env.trivy_docker_fallback }}" != "true" ]; then
            trivy fs --format json --output sca.json . || echo "[]" > sca.json
          else
            docker run --rm -v $(pwd):/workspace -w /workspace aquasec/trivy:0.66.0 fs --format json --output sca.json . || echo "[]" > sca.json
          fi

      - name: Generate SBOM
        continue-on-error: true
        run: |
          export PATH=$HOME/.local/bin:$PATH
          syft dir:app -o json > sbom.json || echo "[]" > sbom.json

      - name: Wait for service to be ready
        run: |
          export KUBECONFIG=kubeconfig
          kubectl wait --for=condition=ready pod -n app-test -l app=demo-app --timeout=300s
          kubectl get service demo-app-svc -n app-test
          kubectl get ingress -n app-test
          kubectl describe ingress demo-app-ing -n app-test

      - name: Create ZAP ConfigMap and Job YAML
        run: |
          export KUBECONFIG=kubeconfig
          mkdir -p scanners
          
          # Create ZAP config
          cat <<EOT > scanners/zap_config.yaml
          # ZAP configuration
          EOT
          
          # Create ConfigMap for ZAP config
          kubectl create configmap zap-config \
            --from-file=zap_config.yaml=scanners/zap_config.yaml \
            -n app-test \
            --dry-run=client -o yaml | kubectl apply -f -
          
          # Create ZAP Job YAML
          cat <<EOT > scanners/zap-job.yaml
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: zap-scan
            namespace: app-test
          spec:
            template:
              spec:
                containers:
                - name: zap
                  image: owasp/zap2docker-stable:2.15.0
                  command:
                  - zap-baseline.py
                  args:
                  - "-t"
                  - "http://demo-app-svc.app-test.svc.cluster.local:80"
                  - "-r"
                  - "/zap/wrk/zap-report.html"
                  - "-J"
                  - "/zap/wrk/zap-report.json"
                  - "-I"
                  volumeMounts:
                  - name: zap-config
                    mountPath: /zap/config
                restartPolicy: Never
                volumes:
                - name: zap-config
                  configMap:
                    name: zap-config
            backoffLimit: 1
          EOT

      - name: DAST scan with OWASP ZAP
        continue-on-error: true
        run: |
          export KUBECONFIG=kubeconfig
          kubectl apply -f scanners/zap-job.yaml
          echo "Waiting for ZAP job to complete"
          kubectl wait --for=condition=complete job/zap-scan -n app-test --timeout=1800s || {
            echo "ZAP scan job failed or timed out"
            kubectl describe job zap-scan -n app-test
            kubectl get pods -n app-test -l job-name=zap-scan
            POD_NAME=$(kubectl get pod -n app-test -l job-name=zap-scan -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
            if [ ! -z "$POD_NAME" ]; then
              kubectl logs -n app-test $POD_NAME || true
            fi
            echo '{"site": []}' > zap-report.json
            exit 0
          }
          POD_NAME=$(kubectl get pod -n app-test -l job-name=zap-scan -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
          if [ ! -z "$POD_NAME" ]; then
            kubectl cp app-test/$POD_NAME:/zap/wrk/zap-report.json zap-report.json 2> zap_cp_error.log || {
              echo "Error copying zap-report.json"
              cat zap_cp_error.log
              echo '{"site": []}' > zap-report.json
            }
            kubectl cp app-test/$POD_NAME:/zap/wrk/zap-report.html zap-report.html 2> zap_html_cp_error.log || {
              echo "Error copying zap-report.html"
              cat zap_html_cp_error.log
              echo "<html><body>No ZAP report generated</body></html>" > zap-report.html
            }
          else
            echo "No ZAP pod found, creating empty reports"
            echo '{"site": []}' > zap-report.json
            echo "<html><body>No ZAP report generated</body></html>" > zap-report.html
          fi
          # Ensure we have valid JSON
          if [ -f zap-report.json ]; then
            cat zap-report.json
          fi

      - name: Merge findings
        run: |
          source venv/bin/activate
          python3 risk_model/inference/merge_findings.py \
            --sast sast.json \
            --secrets secrets.json \
            --iac checkov-report.json \
            --sca sca.json \
            --dast zap-report.json \
            --out findings.jsonl
          deactivate
          echo "Debug: Merged Findings"
          cat findings.jsonl || echo "No findings.jsonl found"

      - name: AI inference + HTML report
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          source venv/bin/activate
          python3 risk_model/inference/infer.py \
            --artifacts-dir artifacts \
            --in findings.jsonl \
            --out enriched.jsonl \
            --report report.html
          deactivate
          echo "Debug: Input Findings"
          cat findings.jsonl || echo "No findings.jsonl found"
          echo "Debug: Enriched Findings"
          cat enriched.jsonl || echo "No enriched.jsonl found"
          echo "Debug: Report"
          cat report.html || echo "No report.html found"

      - name: Evaluate adaptive gate
        id: gate
        run: |
          source venv/bin/activate
          python3 adaptive/evaluate_gate.py > gate.json
          THRESHOLD=$(jq -r .adaptive_threshold gate.json)
          echo "threshold=$THRESHOLD" >> $GITHUB_OUTPUT
          echo "Adaptive threshold set to: $THRESHOLD"
          deactivate

      - name: Check gate
        env:
          THRESH: ${{ steps.gate.outputs.threshold }}
        run: |
          source venv/bin/activate
          python3 adaptive/check_gate.py
          deactivate

      - name: Run Red Team tests
        continue-on-error: true
        run: |
          export KUBECONFIG=kubeconfig
          source venv/bin/activate
          TARGET_HOST=$(kubectl get ingress -n app-test -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "staging.hamza-builds.info")
          python3 redteam/run_attacks.py --target http://$TARGET_HOST:80 --out redteam_results.json || echo "[]" > redteam_results.json
          deactivate

      - name: Verify App Accessibility
        continue-on-error: true
        run: |
          # Try HTTP first, then HTTPS with insecure flag
          curl --fail http://staging.hamza-builds.info || {
            echo "HTTP failed, trying HTTPS with insecure flag"
            curl --fail --insecure https://staging.hamza-builds.info || {
              echo "App not accessible at staging.hamza-builds.info"
              echo "Checking certificate and cluster issuer status..."
              kubectl describe certificate demo-app-tls -n app-test || echo "Certificate not found"
              kubectl describe clusterissuer letsencrypt-prod || echo "ClusterIssuer not found"
              echo "This might be expected if cert-manager is not properly configured"
              exit 0  # Don't fail the workflow for SSL issues
            }
          }

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-report-${{ github.run_number }}
          path: |
            report.html
            findings.jsonl
            enriched.jsonl
            redteam_results.json
            zap-report.json
            zap-report.html
            checkov-report.json
            sast.json
            secrets.json
            sca.json
            sbom.json
            gate.json

      - name: Clean up kubeconfig
        if: always()
        run: |
          rm -f kubeconfig
